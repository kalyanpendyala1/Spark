{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames are immutable distributed collection of data that is organized like a table in relational database. Spark DataFrames are similar to pandas dataframes. \n",
    "\n",
    "In Spark 2.0 we are using `SparkSession` instead of SQLContext to work with DataFrames. The various Spark contexts like HiveContext, SQLContext, StreamingContext and SparkContext are merged together into SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalyst Optimizer is similar to DAG scheduler in execution or RDD. Catalyst Optimizer finds the most efficient plan to execute data operations.\n",
    "\n",
    "As opposed to immediately processing the query, the spark engine's Catalyst Optimizer compiles and optimizes logical plan (series of algebraic and logical constructs like SELECT, GROUPBY represented in tree like structure) and has a cost optimizer that determines h\\the most efficient physical plan (contains series of low level functions represented as a tree like structure) generated.\n",
    "\n",
    "- It helps to requce the query time.\n",
    "\n",
    "- It is written in scala and as a developer you can write you own optimization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why DataFrames are faster than RDDs\n",
    "\n",
    "- RDDs are not optimized as DataFrames optimized with Catalyst Optimizer.\n",
    "\n",
    "- Communication between python and JVM made RDDs very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame\n",
    "\n",
    "Typically, you will create DataFrames by importing data using SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a JSON file\n",
    "# creating RDD out of this JSON file\n",
    "JsonRDD = sc.parallelize((\"\"\"\n",
    "{ \"id\": \"123\",\n",
    "\"name\": \"kalyan\",\n",
    "\"age\" : 25,\n",
    "\"eyeColor\" : \"brown\"}\n",
    "\"\"\",\n",
    "                         \n",
    "\"\"\"\n",
    "{ \"id\": \"124\",\n",
    "\"name\": \"Varun\",\n",
    "\"age\" : 30,\n",
    "\"eyeColor\" : \"black\"}\n",
    "\"\"\",\n",
    "                         \n",
    "\"\"\"\n",
    "{ \"id\": \"125\",\n",
    "\"name\": \"Shiva\",\n",
    "\"age\" : 35,\n",
    "\"eyeColor\" : \"blue\"}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "|age|eyeColor| id|  name|\n",
      "+---+--------+---+------+\n",
      "| 25|   brown|123|kalyan|\n",
      "| 30|   black|124| Varun|\n",
      "| 35|    blue|125| Shiva|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert this RDD into DataFrame by using SparkSession read.json method\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "df = spark.read.json(JsonRDD)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=25, eyeColor='brown', id='123', name='kalyan'),\n",
       " Row(age=30, eyeColor='black', id='124', name='Varun'),\n",
       " Row(age=35, eyeColor='blue', id='125', name='Shiva')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to execute SQL queries create temporary table first\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"select * from df\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Number of Rows in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No. of Rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Filter statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|124| 30|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get id, age where age = 30\n",
    "\n",
    "df.select(\"id\", \"age\").filter(\"age = 30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|124| 30|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way\n",
    "\n",
    "df.select(df.id, df.age).filter(df.age == 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Varun|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wildcards\n",
    "\n",
    "df.select(df.name).filter(\"eyeColor like 'bla%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying dataframe with SQL statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running filter statements using where clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|124|Varun|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, name from df where age = 30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
