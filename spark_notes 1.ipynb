{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Spark?\n",
    "\n",
    "Spark is open-source distributed querying and processing engine. It allows user to read, transform and aggregate data, as well as train and deploy sophisticated statistical models. It has different APIs like\n",
    "\n",
    "- MLlib and ML for machine learning\n",
    "\n",
    "- GraphX and GraphFrames for graph processing\n",
    "\n",
    "- Spark Streaming (DStreams and Structured)\n",
    "\n",
    "- SparkSQL\n",
    "\n",
    "It can read and write from diverse data sources including but not limited to HDFS, Cassandra, HBase and S3. \n",
    "\n",
    "Spark supports Java, Python, Scala and R. Tasks most frequently associated with spark include ETL and SQL batch jobs across large datasets.\n",
    "\n",
    "SparkSession is unified entry point into spark application. Spark session is combination of different contexts like \"Spark Context\", \"hive context\", \"SQL context\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Execution process\n",
    "\n",
    "<img src=\"datasets/exec.png\">\n",
    "\n",
    "\n",
    "Any Spark application spins off a single driver process (that contain multiple jobs) on the master node that then directs executor processes (that contain multiple tasks) distributed to a number of worker nodes. The driver node determines the number and composition of the task processes based on the graph generated for the given job.\n",
    "\n",
    "https://spark.apache.org/docs/latest/cluster-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark vs MapReduce\n",
    "\n",
    "Alternative to MapReduce.\n",
    "\n",
    "MapReduce writes most of the data to disk after each map and reduce operation, spark keeps data in-memory after each transformation. Spark can spill over data to disk if the memory is filled.\n",
    "\n",
    "Spark can use data stored in variety of formats, and databases like Cassandra, S3, HDFS etc., Where as MapReduce can access data stored only in HDFS.\n",
    "\n",
    "MapReduce is only for Batch processing. Spark works for Batch processing as well as real time processing.\n",
    "\n",
    "MapReduce is slower than spark because of its I/O latency. Spark 100x faster in-memory and 10x faster on disk.\n",
    "\n",
    "MapReduce is Data processing engine. Spark is Data analytics engine\n",
    "\n",
    "Supports SQL through HiveSQL -- Supports SQL through spark SQL\n",
    "\n",
    "MapReduce is not interactive -- Spark is interactive\n",
    "\n",
    "More lines of code -- Less lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Spark\n",
    "\n",
    "`In-Memory Computing`: Keeping data in server's RAM as it makes is easy to access data and makes Machine learning algorithms to work faster.\n",
    "\n",
    "`Lazy Evaluation` Execution will not start until action is triggered.\n",
    "\n",
    "Supports `Multiple Languages`: Spark allows you to write applications on Java, Python, Scala and R.\n",
    "\n",
    "`100x faster`\n",
    "\n",
    "`Advanced Analytics`: Spark not only supports 'Map' and 'Reduce' it also supports SQL queries, Streaming data, Machine Learning Algorithms, and Graph algorithms.\n",
    "\n",
    "`Real-Time` Processing Spark can handle real-time processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between Pandas DataFrame and Spark DataFrames\n",
    "\n",
    "\n",
    "The key difference between pandas and spark dataframes is eager and lazy evaluation. \n",
    "\n",
    "Example: You can specify operation using Spark DataFrame for loading dataset from S3 and applying no. of transformations to the dataframe, but the operations wont immediately applied, Instead a graph transformation is recorded. and once the data is actually needed the transformations are applied. This approch avoids pulling full dataframe into the memory. With pandas Dataframes everything is pulled into the memory and every transformation is applied immediately.\n",
    "\n",
    "when reading CSV files into the spark dataframes, spark performs the operations in eager mode, meaning that all the data is loaded into memory before next step begins execution. While lazy evaluation is used when reading files in paraquet format. large CSV files must be transformed into paraquet before executing pipeline. \n",
    "\n",
    "Spark output is stored to S3 in the form of paraquet files, or it can be directly sent to NoSQL databases.The best and easy way is to store output in S3 and NoSQL database will access from S3.\n",
    "\n",
    "#### Differences b/w pandas and spark\n",
    "\n",
    "In pandas you can easily read csv files directly using 'read_csv()'. But Spark supports professional formats like JSON, paraquet, Hive tables, and can read from S3, HDFS and local or RDBMS but CSV is not supported natively in spark. You can use library 'Spark-csv'.\n",
    "\n",
    "The other differences are difference is methods like head(), describe(), count() etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Client mode and cluster mode\n",
    "\n",
    "https://www.youtube.com/watch?v=vJ0eUZxF80s&list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&index=4\n",
    "https://www.youtube.com/watch?v=fyTiJLKEzME&list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&index=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Dataset\n",
    "\n",
    "RDDs are collection of JVM objects. RDDs are predominantly stored in memory. These are most fundamental data object used in Spark. Python data is stored in these JVM objects. Most spark programming consists of creating new RDDs by performing operations on existing RDDs.\n",
    "\n",
    "\n",
    "*`Resilient:`* RDDs are resilient, meaning that if a node performing an operation in Spark is lost, the dataset can be reconstructed. This is because Spark knows the *lineage* of each RDD, which is the sequence of steps to create RDD.\n",
    "\n",
    "\n",
    "*`Distributed:`* RDDs are distributed, meaning the data in RDDs is divided into one of many *partitons* and distributed in-memory collections of objects across worker nodes in the cluster.\n",
    "\n",
    "\n",
    "*`Dataset:`* RDDs are datasets that consists of *records*. Records are uniquely identifiable data collections within a dataset. Records could be similar to rows in relational database, a line of text, etc., RDDs are partitioned such that each partition contains a unique set of records and can be operated independently.\n",
    "\n",
    "Another key property of RDDs is their *immutability*, which means that after they are instantiated and populated with data, they cannot be updated. Instead, new RDDs are created by performing transformations such as map or filter functions on existing RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark-2.4.4-bin-hadoop2.6\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import findspark\n",
    "findspark.init('C:\\spark-2.4.4-bin-hadoop2.6')\n",
    "import pyspark\n",
    "print(findspark.find())\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark context\n",
    "# SparkContext(sc) is the entry point into spark cluster.  \n",
    "# we use sc object to perform file read operation and then collect the data.\n",
    "conf = pyspark.SparkConf().setAppName('sparkApp').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs from textfiles\n",
    "\n",
    "**textFile():** Reads a text file from HDFS or local file system and return it as an RDD of strings.\n",
    "\n",
    "**wholeTextFiles():** Reads a directory of text files, rather than creating basic RDD, it returns pairRDD with filename with path as key, and value being the whole file as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# reading file into RDD. \n",
    "# textFile() method creates RDD with each line as an element.\n",
    "\n",
    "lines = sc.textFile('datasets/days.txt')\n",
    "print(\"Type: \",type(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " you cant able to view contents of RDD using print function. collect() is an action that returns all the elements of an RDD as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days\n",
      "Monday\n",
      "Tuesday\n",
      "Wednesday\n",
      "Thursday\n",
      "Friday\n",
      "Saturday\n",
      "Sunday\n"
     ]
    }
   ],
   "source": [
    "col = lines.collect() # returns an array. You can print it.\n",
    "\n",
    "# printing elements one by one\n",
    "for line in col:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method used to check the number of paritions\n",
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts no. of lines in a text file. \n",
    "# if your partitions are located in different nodes. It counts no. of lines in all the partitions\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.api.java.JavaPairRDD@6dcec36f"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = sc.wholeTextFiles('datasets/*.txt')\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the key difference between the two approaches. Partitions were created using the textFile() method to load the same data as there were different files. The wholeTextFiles() method combines all of the files (which are quite small in this case) into a single partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keys() method creates a new RDD named filenames containing only a list of keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/D:/Notebooks/Spark/datasets/days.txt',\n",
       " 'file:/D:/Notebooks/Spark/datasets/untitled.txt']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = pairs.keys()\n",
    "filenames.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Days\\nMonday\\nTuesday\\nWednesday\\nThursday\\nFriday\\nSaturday\\nSunday']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filedata = pairs.values()\n",
    "filedata.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Days\\nMonday\\nTuesday\\nWednesday\\nThursday\\nFriday\\nSaturday\\nSunday',\n",
       " 'January\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filedata.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating a RDD from a Datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the data from external sources lke Oracle, MySQL, Postgres, SQL server etc., into an RDD, it will attempt to partition the data into multiple partitions across multiple workers by dividing the table into different partions. These partitions can be loaded in parallel and each partition is responsible for fetching unique set of rows.\n",
    "\n",
    "\n",
    "The preferred methods of creating an RDD from a relational database table or query use functions from a special SparkContext called **SQLContext.** The SQLContext is spark's entry point for working with tabular data. DataFrames are created using SQL context.\n",
    "\n",
    "Launch pyspark supplying JDBC database connector, Then initialize SQLContext, From pyspark shell we use load() method to load data from the table and create RDD from JDBC datasource. But the preferred method is to use DataFrameReader and create DataFrames.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDD from Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallelize is used to create RDD out of a collection\n",
    "rdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first element of RDD\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get minimum of RDD\n",
    "rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get maximum of RDD\n",
    "rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first 5 elements of RDD\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get no. of partitions\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to get all the data to the driver we use collect()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newrdd = rdd.filter(lambda x:x%2)\n",
    "newrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newrdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of RDDs\n",
    "\n",
    "*`PairRDD`*: RDD of key value pairs.\n",
    "\n",
    "*`DoubleRDD`*: RDD consisting of a collection of double values only. Because the values are of same numeric type, several additional statistical functions are available, including mean(), sum(), stdev, variance and histogram etc.,\n",
    "\n",
    "*`DataFrame`*: A distributed collection of data organized into named columns. DataFrame is equivalent to relational table.\n",
    "\n",
    "*`SequenceFileRDD`*: RDD created from sequence file\n",
    "\n",
    "*`ShuffledRD`*D: Resulting RDD from shuffle\n",
    "\n",
    "*`UnionRDD`*: Resulting RDD from union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paired RDD\n",
    "\n",
    "Paired RDDs are RDDs containing a key-value pair. Where Key is the identifier and value is the corresponding value. Pair RDDs helps in \"Shuffle\" operations such as grouping and aggregating the elements by a key.\n",
    "\n",
    "We can also use join() method to merge the two RDDs together by grouping the elements with same key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Days', 1),\n",
       " ('Monday', 1),\n",
       " ('Monday', 1),\n",
       " ('Monday', 1),\n",
       " ('Monday', 1),\n",
       " ('Tuesday', 1),\n",
       " ('Wednesday', 1),\n",
       " ('Wednesday', 1),\n",
       " ('Wednesday', 1),\n",
       " ('Wednesday', 1),\n",
       " ('Thursday', 1),\n",
       " ('Friday', 1),\n",
       " ('Saturday', 1),\n",
       " ('Sunday', 1),\n",
       " ('Sunday', 1),\n",
       " ('Sunday', 1)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('datasets/days.txt')\n",
    "pairs = lines.map(lambda x: (x,1))\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Days', 1),\n",
       " ('Monday', 4),\n",
       " ('Tuesday', 1),\n",
       " ('Wednesday', 4),\n",
       " ('Thursday', 1),\n",
       " ('Friday', 1),\n",
       " ('Saturday', 1),\n",
       " ('Sunday', 3)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey returns pairs where values for each key are aggregated using reduce function\n",
    "from operator import add\n",
    "add = pairs.reduceByKey(add)\n",
    "add.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Days',\n",
       " 'Monday',\n",
       " 'Tuesday',\n",
       " 'Wednesday',\n",
       " 'Thursday',\n",
       " 'Friday',\n",
       " 'Saturday',\n",
       " 'Sunday']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting distinct elements\n",
    "dis = lines.distinct()\n",
    "dis.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Days', 1),\n",
       " ('Friday', 1),\n",
       " ('Monday', 4),\n",
       " ('Saturday', 1),\n",
       " ('Sunday', 3),\n",
       " ('Thursday', 1),\n",
       " ('Tuesday', 1),\n",
       " ('Wednesday', 4)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByValue counts how many times each value is repeating\n",
    "sorted(lines.countByValue().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Evaluation\n",
    "Lazy evaluation defers processing until an action is called. After an action such as count() or saveAsTextFile() is requested, a DAG is created along with logical and physical execution plans. These are then orchestrated and managed across executors by the driver.\n",
    "https://medium.com/analytics-vidhya/being-lazy-is-useful-lazy-evaluation-in-spark-1f04072a3648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting\n",
    "\n",
    "https://stackoverflow.com/questions/26870537/what-is-the-difference-between-cache-and-persist\n",
    "\n",
    "RDDs are created and exist in memory on executors. By default, RDDs are transient objects that last only while they are required. Once they are transformed into new RDDs and no longer needed for any other operations, they are removed permanantly. \n",
    "\n",
    "This may be problematic if an RDD is required for more than one action because it must be reevaluated entirely each time. An option to address this to use cache or persist.\n",
    "\n",
    "We cache RDDs in cases when,\n",
    "\n",
    "- Reusing then in interative loop (Ml algorithms).\n",
    "\n",
    "- Reuse the RDD multiple times in a single application, job or notebook\n",
    "\n",
    "There were two methods for RDD persistence,\n",
    "\n",
    "- persist()\n",
    "\n",
    "- cache()\n",
    "\n",
    "when you use these methods, RDD will be kept in memory on all the nodes in the cluster where it is computed after the first action is called on it.\n",
    "\n",
    "\n",
    "Cache() function is default of persistence with MEMORY_ONLY storage. But with persist(), you can specify which storage level you want.  So cache() is the same as calling persist() with the default storage level. When you call persist() without any argument it is equivalent to cache.\n",
    "\n",
    "cache() is not ideal for datasets larger than available cluster memory. In that case persist(StorageLevel.MEMORY_AND_DISk_ONLY) is best, which will spill the RDD partitions to worker's local disk if the memory is full.\n",
    "\n",
    "\n",
    "<img src=\"datasets/storage_level.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %s elements in the collection %s (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# persisting the RDD and retrieving\n",
    "\n",
    "originalrdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])\n",
    "newrdd = originalrdd.filter(lambda x: x+2)\n",
    "newrdd.persist()  # persists newrdd to memory\n",
    "no_elements = newrdd.count()\n",
    "list_of_elements = newrdd.collect()\n",
    "\n",
    "# does not have to recompute newrdd\n",
    "print(\"There are %s elements in the collection %s\",(no_elements, list_of_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %s elements in the collection %s (10, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "originalrdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])\n",
    "newrdd = originalrdd.filter(lambda x: x+2)\n",
    "no_elements = newrdd.count()\n",
    "list_of_elements = newrdd.collect()\n",
    "\n",
    "# It reprocess newrdd again\n",
    "print(\"There are %s elements in the collection %s\",(no_elements, list_of_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Lineage\n",
    "\n",
    "RDD Lineage is sequence of transformations that resulted in RDD. Every RDD operation recomputes the entire lineage by default unless RDD is persisted. \n",
    "\n",
    "In an RDDs lineage, each RDD will have a parent RDD and/or child RDD. Spark creates a DAG (directed acyclic graph) consisting of dependencies between RDDs. RDDs are processed in stages, which are sets of transformations. RDDs and stages have dependencies that can be narrow or wide.\n",
    "\n",
    "\n",
    "A shuffle can occur when the resulting RDD depends on other elements from the same RDD or another RDD. \n",
    "\n",
    "Narrow Dependencies: Each partition of the parent RDD is used by atmost one partition of child RDD. No shuffle necessary. Transformations which have narrow dependencies are fast.\n",
    "\n",
    "Wide dependencies are when each partition of the parent RDD may be used by multiple child partitions. Shuffle is necessary and they are slow!.!\n",
    "\n",
    "<img src=\"datasets/nvs.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"datasets/narrow.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"datasets/wide.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "The two types of operations in spark are Transformation and Action.\n",
    "\n",
    "RDDs are immutable, to perform any operation on RDD we need to create a new RDD. Transformation is a function that produces new RDD from existing RDD. It does not modify the RDD that you apply the transformation on, rather it creates a new RDD. Transformations are not executed until an action is called. \n",
    "\n",
    "Actions return values or data to the Driver program. A series of transformations takes place when action is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations\n",
    "\n",
    "- Map\n",
    "\n",
    "- Filter\n",
    "\n",
    "- FlatMap\n",
    "\n",
    "- GroupBy\n",
    "\n",
    "- GroupByKey\n",
    "\n",
    "- MapPartitions\n",
    "\n",
    "- MapPartitionwithIndex\n",
    "\n",
    "- ReduceByKey\n",
    "\n",
    "- Sample\n",
    "\n",
    "- Union\n",
    "\n",
    "- Join\n",
    "\n",
    "- Distinct\n",
    "\n",
    "- Coalesce\n",
    "\n",
    "- KeyBy\n",
    "\n",
    "- PartitionBy\n",
    "\n",
    "- zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "- Collect\n",
    "\n",
    "- Take\n",
    "\n",
    "- Reduce\n",
    "\n",
    "- Aggregate\n",
    "\n",
    "- Max\n",
    "\n",
    "- Sum\n",
    "\n",
    "- Mean\n",
    "\n",
    "- Stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling transformations\n",
    "\n",
    "During development and discovery, we need to sample the data in RDDs before running a process across the whole input data. Spark provides several functions to sample RDDs and produce new RDDs.\n",
    "\n",
    "The **sample** transformation is used to create a sampled subset RDD from original RDD based upon a percentage of overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(range(0,1000))\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample transformation\n",
    "sampled_data = data.sample(withReplacement=False, fraction=0.1, seed=None)\n",
    "sampled_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **takeSample** action is used to return a random list of values (elements or values) from the RDD being sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[894, 664, 71, 857, 626, 776, 673, 536, 891, 318]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takeSample action\n",
    "sample2 = data.takeSample(False, 10, 1)\n",
    "print(len(sample2))\n",
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Transformations\n",
    "\n",
    "Functional transformation include mapping and filtering functions. The **map** transformation is the most basic of all transformations. It applies a function to every element of RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "Original:  [8, 11, 32, 38, 42]\n",
      "+ 2:  [10, 13, 34, 40, 44]\n"
     ]
    }
   ],
   "source": [
    "# Map Transformation\n",
    "print(type(sampled_data))\n",
    "print(\"Original: \",sampled_data.take(5))\n",
    "newRdd = sampled_data.map(lambda x : x+2)\n",
    "print(\"+ 2: \",newRdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flatMap** is similar to map, it returns a new RDD by applying a function to each element of the RDD but output is flattened. Also, function in flatMap can return a list of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([3,4,5])\n",
    "rdd2 = rdd1.flatMap(lambda x: [[x , x*x]])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **filter** transformation evaluates a Boolean expression against each element(record) in the dataset. The Boolean value returned determines whether or not the record is included in the resultant ouput RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KALYAN,', 'SPARK', 'NOTES,', 'FLATMAP', 'TRANSFORMATIONS']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = sc.textFile('datasets/intro.txt')\n",
    "print(type(txt))\n",
    "\n",
    "filtered = txt.map(lambda x: x.upper())     \\\n",
    "              .flatMap(lambda x: x.split()) \\\n",
    "              .filter(lambda x: len(x)>4)\n",
    "\n",
    "\n",
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping, Sorting and Distinct Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 4, 6, 8, 8, 8, 8, 8]), (1, [1, 3, 7, 9])]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupby\n",
    "rdd = sc.parallelize([1,2,8,9,3,8,8, 8, 8,6,4,7])\n",
    "result = rdd.groupBy(lambda x : x%2).collect()\n",
    "sorted([(x, sorted(y)) for (x,y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 7, 8, 8, 8, 8, 8, 9]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort\n",
    "sort = rdd.sortBy(lambda x:x)\n",
    "sort.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct\n",
    "sort.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Operations\n",
    "\n",
    "The **union** transformation takes one RDD and appends it to another RDD resulting in a combined RDD. The RDDs are not required to have same schema or structure. The union transformation doesnt filter duplicates. To filter duplicates, you could follow the union transformation with the distinct function. \n",
    "\n",
    "The **intersection** transformation returns elements that are present in both RDDs.\n",
    "\n",
    "The **subtract** transformation returns all elements from the first RDD that are not present in second RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union\n",
    "odds = sc.parallelize([1,3,5,7,9])\n",
    "even = sc.parallelize([0,2,4,6,8])\n",
    "odds.union(even).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 7]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection\n",
    "set_a = sc.parallelize([1,2,3,5,7])\n",
    "set_b = sc.parallelize([1,2,4,6,7,8,9,10])\n",
    "set_b.intersection(set_a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtract\n",
    "set_a.subtract(set_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Actions\n",
    "\n",
    "Actions in spark return values to the Spark Driver program. With Lazy evaluation, the complete set of Spark transformations in a program are only processed when an action is requested.\n",
    "\n",
    "The **count** action takes no arguments, it returns the number of records in RDD.\n",
    "\n",
    "The **collect** action returns a list that contains all the elements in RDD to Spark driver. Note: Collect doesnt restrict the output, which can be quite lrage and can potentially cause out-of-memory errors on the driver, it is typically used for small RDDs or development.\n",
    "\n",
    "The **take** action returns first n elements of RDD. The elements are not in any particular order; they can differ if the same action is run again. *takeOrdered*, is the another function which takes the first n elements based upon a key supplied by a key function.\n",
    "\n",
    "The **top** action returns the top n elements from an RDD, but unlike *take*, the elements are orderd and returned in descending order. Order is determined by object type, such as numerical order for integers and dictionary order for strings.\n",
    "\n",
    "The **first** action returns the first element in RDD.\n",
    "\n",
    "The **reduce** is aggregate action. There are two important properties that an aggregation function should have.\n",
    "\n",
    "*Commutative:* (A+B = B+A) result would be independent of order of elements.\n",
    "\n",
    "*Associative:* (A+B)+C = A+(B+C) ensuring any two elements associated in the aggregation at a time does not affect the final result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "rdd1 = sc.textFile('datasets/days.txt')\n",
    "rdd2 = rdd1.flatMap(lambda x : x.split())\n",
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Days',\n",
       " 'Monday',\n",
       " 'Monday',\n",
       " 'Monday',\n",
       " 'Monday',\n",
       " 'Tuesday',\n",
       " 'Wednesday',\n",
       " 'Wednesday',\n",
       " 'Wednesday',\n",
       " 'Wednesday',\n",
       " 'Thursday',\n",
       " 'Friday',\n",
       " 'Saturday',\n",
       " 'Sunday',\n",
       " 'Sunday',\n",
       " 'Sunday']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Days', 'Monday']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take\n",
    "rdd2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wednesday', 'Wednesday']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top\n",
    "rdd2.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first\n",
    "rdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is:  21\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "rdd1 = sc.parallelize([1,2,3,4,5,6])\n",
    "sum1 = rdd1.reduce(lambda a,b : a+b)\n",
    "print(\"The sum is: \", sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Days', 1),\n",
       " ('Monday', 4),\n",
       " ('Tuesday', 1),\n",
       " ('Wednesday', 4),\n",
       " ('Thursday', 1),\n",
       " ('Friday', 1),\n",
       " ('Saturday', 1),\n",
       " ('Sunday', 3)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey\n",
    "\n",
    "from operator import add\n",
    "rdd1 = sc.textFile('datasets/days.txt')\n",
    "rdd2 = rdd1.map(lambda x: (x,1))\n",
    "rdd2.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
